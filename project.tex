\documentclass[12pt,abstracton]{scrartcl}
%packages
\usepackage{bbm}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{texdraw}
\usepackage{euscript}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{mathrsfs}
\usepackage{textcomp}
\usepackage{color}
\usepackage{fancyvrb}
\usepackage{pstricks,pst-node,pst-tree}
\usepackage{subfig}
\usepackage{tikz}
\usepackage{url}
\usepackage{array}
%margins
\setlength{\textwidth}{6.5in} \setlength{\evensidemargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\textheight}{9in}
%section start markers
\newcommand{\lem}{\paragraph*{Lemma:}}
\newcommand{\cor}{\paragraph*{Corollary:}}
\newcommand{\prb}[1]{\section*{Problem {\text{#1}:}}}
\newcommand{\sprb}[1]{\subsection*{Item {\text{#1}:}}}
%abbreviations
\newcommand{\fa}{\forall}
\newcommand{\EE}{\exists}
\newcommand{\ee}{\epsilon}
\newcommand{\dd}{\delta}
\newcommand{\rr}{\mathbb{R}}
\newcommand{\rrx}{\rr^{\times}}
\newcommand{\qq}{\mathbb{Q}}
\newcommand{\nn}{\mathbb{N}}
\newcommand{\zz}{\mathbb{Z}}
\newcommand{\cc}{\mathbb{C}}
\newcommand{\ccx}{\cc^{\times}}
\newcommand{\oo}{\mathbb{O}}
\newcommand{\hh}{\mathbb{H}}
\newcommand{\pp}{\mathbb{P}}
\newcommand{\prm}[1]{#1^{\prime}}
\newcommand{\Letf}[3]{Let \({#1}:{#2}\rightarrow{#3}\) be}
\newcommand{\pard}[2]{\frac{\partial {#1}}{\partial {#2}}}
\newcommand{\FIT}{First Isomorphism Theorem}
\newcommand{\img}{\mathrm{img}\ }
\newcommand{\eq}[1]{\begin{equation}#1\end{equation}}
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\ip}[2]{\langle{#1},{#2}\rangle}
\newcommand{\contra}{\Rightarrow\Leftarrow}
\newtheorem{thm}{Theorem}
\newtheorem{clm}{Claim}
%section end markers
\newcommand{\Qed}{\hfill$\square$\newline\newline}
\newcommand{\QED}{\hfill$\square$}
%\renewcommand*\arraystretch{1.1}
\renewcommand*{\thefootnote}{\arabic{footnote}}

\title{Code Like a Pro:\\Observations on Expert ML Code}
\subtitle{Comp150FP Final Project\footnote{This project, advised by Prof. Norman Ramsey, is also submitted in partial fulfillment of the requirements for M.S. in computer science}}
\author{Zhe Lu}
\date{10 December 2013}

\begin{document}
\bibliographystyle{alpha}
\input{epsf.sty}
\maketitle
\begin{abstract}
Beginners can become experts through directed practice towards a goal. In an effort to
help them direct their practice, we make observations about what good ML code looks like.
The aim of this paper is two-fold: (1)~identify features common to code written in Standard ML by experts and
(2)~provide suggestions to help novices model their code after expert code.
\end{abstract}
\section{INTRODUCTION}
There are many ways to define what an expert is.
For example, an often cited rule is that one requires 10000 hours of deliberate practice
in a particular field to become an expert \cite{Gla08}. But if we want to provide novice programmers
with specifics during these 10000 hours, what should we tell them? One challenge is that experts themselves
do not necessarily know what features identify them as experts and what features are unique to them as individuals.

We attempt to shed light onto this problem by employing software metrics,
by trying to find measurable quantities
that provide some metric for qualifying expert code. Specifically,
We approach this problem by examining source code written by experts.

For samples of expert code, we turned to prominent projects where the source code is readily available:
\begin{enumerate}
\item The Standard ML of New Jersey compiler (SML/NJ), whose parser was used extensively for code analysis in this project (299 parseable files, 81102 lines of code)\footnote{\url{http://smlnj.cs.uchicago.edu/dist/working/110.76/compiler.tgz}}
\item The MLKit compiler for SML (1319 parseable files, totalling 306154 lines of code)\footnote{\url{http://www.itu.dk/research/mlkit/dist/mlkit-4.3.0-src.tgz}}
\item FoxNet, an implementation of the standard TCP/IP networking protocol stack in SML (72 parseable files, 8199 lines of code)\footnote{\url{ftp://ftp.cs.cmu.edu/project/fox/snapshot/foxnet.tar.gz}}
\item Til, a type-directed compiler for ML (460 parseable files, totalling 109813 lines of code)\footnote{\url{http://www.cs.cornell.edu/Info/People/jgm/til.tar.Z}}
\item MLton, an open-source, whole-program, optimizing SML compiler (424 parseable files, 58226 lines of code)\footnote{\url{http://sourceforge.net/projects/mlton/files/mlton/20130715/mlton-20130715.src.tgz/download}}
\end{enumerate}

We ran into a few challenges and complications when analyzing the code.
In order to focus on real code, we've chosen to ignore source files that are associated
with testing, regression, or microbenchmarks. On inspection, these files contain much more imperative
code and do not make the same use of the module system than core files of each project.
In addition to this issue, due to the presence of the \texttt{open} declaration and
heavy use of the compilation manager in each project, we had trouble parsing for
infix expressions. Therefore, we performed our analysis on measurable features
of the pre-infix parse tree obtained from the SML/NJ ``Visible Compiler.''

The layout of this paper is as follow.
Section~\ref{sec:metric} provides a brief background
concerning software metrics and source code analysis.
We look at how well expert code conforms to readily available
style guides in section~\ref{sec:style}.
Section~\ref{sec:syntax} discusses an analysis
of the distribution of syntactic forms.
Finally, we conclude and reflect in section~\ref{sec:future}.
\section{SOFTWARE METRICS AND SOURCE CODE ANALYSIS}\label{sec:metric}
\subsection{SOFTWARE METRICS}
One way to tackle the challenges and complexities of software development is to employ
\emph{software metrics}. These metrics allow developers and their managers to make
measurements about cost and effort, productivity, assessment of reliability and
quality, evaluating processes (including development and maintenance), and elements
related to project and product management \cite{Ord08}. Despite being around since
the early 1970s, there are still some significant issues with how measurements
about software are made in practice and serious criticisms about whether or not the metrics
measure anything useful (for example, see: \cite{Jon94} and \cite{Bou12}).
Nevertheless, due to the high potential of prudent measurements,
a growing number of organizations have integrated software measurement programs into
their workflows \cite{Ord08}.

For our project, the whole of software metrics is too monolithic to employ on
such a small-scale project. Therefore, we aim to look at a more specific set of measurements,
mostly dealing with style. This approach has been used for C++ \cite{Aye98} and C \cite{Tak11}.
\subsection{SOURCE CODE ANALYSIS}
In an effort to understand what good code looks like, we analyze source code
written by experts. \cite{Bin07} provides a definition:
\begin{quote}
\emph{Source code analysis} is the process of extracting
information about a program from its source code
or artifacts (e.g., from Java byte code or execution
traces) generated from the source code using automatic tools.
\emph{Source code} is any static, textual,
human readable, fully executable description of
a computer program that can be compiled automatically into an executable form.
\end{quote}

According to \cite{Bin07}, source code analysis has three components: a parser,
the internal representation, and the analysis of this representation.
Internal representations are many and varied, including abstract syntax trees,
control-flow graphs, and call graphs.

Source code analysis has a variety of applications in such areas as
automotive software engineering, debugging, fault location, software maintenance,
etc \cite{Bin07}.

We use the parser of the Standard ML of New Jersey compiler along with the accompanying
abstract syntax tree that it produces. Our analysis
consists of static analyses -- that is, ignoring program input -- of the abstract syntax tree, looking for
syntactic constructs and patterns in usage.
\section{STYLE}\label{sec:style}
\subsection{Style Guides}\label{subsec:guide}
If we wish to examine how well expert code conforms to style, we need to decide
what good style is. A quick search of the web yielded three style guides, from which
we will distill a few points of style to examine in the expert code:
\begin{enumerate}
\item CS312 SML Style Guide, from Cornell\footnote{\url{http://www.cs.cornell.edu/courses/cs312/2008sp/handouts/style.htm}}
\item SML Style Guide, from CMU\footnote{\url{http://www.cs.cmu.edu/~15150/resources/style.pdf}}
\item Syntactic Conventions, from MLton website\footnote{\url{http://mlton.org/SyntacticConventions}}
\end{enumerate}

We cannot take these style guides at face value. Rather, we use
the style guides as starting points, whose suggestions are potential
features of analysis.
\subsection{Tabs vs. Spaces}\label{subsec:tab}
If one wishes to start a flame war on StackOverflow, one simply needs to start
a debate over spaces and tabs. The three style guides, nevertheless, are
united on the view that tabs are taboo in SML code. To paraphrase the CMU
style guide, white space should emphasize the structure of the code \cite{Cmu12}.
Tabs, which are potentially rendered differently for
different users, cannot be used to consistently provide the desired indentation.
For some users, this ability to customize the size of tabs is an asset not
a drawback. These two view points generally underlie all ``debates'' concerning spaces versus tabs.

We can see from Table~\ref{table:whitespace} that most expert code definitely does
not conform to the tab/spaces rule -- we see a tab character approximately once every third line.
It seems that the coders made a decision to use tabs and stuck to the decision fairly well.
The notable exception is complete lack of tabs in MLton, whose style guide we are using as an example.
It's good to see that the principled experts at MLton stick to their own guidelines.
But based on the fiery nature of the debate over the proper character
for indenting, perhaps it is not surprising that the tab/spaces
rule from style guides is adopted by only a select group of coders.
While we can't say that the experts choose one style over the other in accordance with
any style guide, we can say that the experts are at least consistent, which is
a potentially helpful observation we can pass on to novice coders.
\subsection{The 80-Column Rule}\label{subsec:80}
All three style guides recommend a line of code should not exceed 80 columns.
The 80-column rule is a UNIX convention and many text editors automatically
wrap at 80 columns \cite{Cmu12}.

\begin{table}[t!]
\centering
\begin{tabular}{|l||c|c||c|c|}
\hline
Project & Tab violations & Lines per instance & Column violations & Lines per instance \\ \hline\hline
SML/NJ & 23115 & 3.5086 & 350 & 231.72 \\
MLKit & 100827 & 3.0364 & 13857 & 22.094 \\
FoxNet & 2708 & 3.0277 & 31 & 264.48 \\
Til & 52522 & 2.0908 & 782 & 140.43 \\
MLton & 0 & -- & 713 & 81.663 \\ \hline
\end{tabular}
\caption{Style violations involving whitespace}
\label{table:whitespace}
\end{table}

The example code from the expert ML users we looked at clearly cannot
resist violating the 80-column rule (Table~\ref{table:whitespace}), with
those for MLKit being particularly egregious. A quick glance
at the violations shows that most of the cases consist of exceeding the limit
by only a few characters. While most editors can be configured to insert
spaces in place of tabs, there is no active check on enforcing the 80-column
rule while coders are working.
Indeed, MLton contains many width violations in spite of what is stated in their style guide.
Since the level of indentation does more to highlight the structure of code than
limiting the width of text does, it is perhaps not surprising that more
emphasis was placed on eliminating tabs than on observing the 80-column rule.

One possible suggestion is that not all projects enforce the same width limit.
We plotted the fraction of lines at least as wide as a given cutoff for each project,
and we examined limits from 70 columns to 100 columns, inclusive.

\begin{figure}[h!]
\centering
\begin{tabular}{cc}
\includegraphics[scale=0.72]{smlnj.eps} & \includegraphics[scale=0.72]{mlkit.eps} \\
\includegraphics[scale=0.72]{foxnet.eps} & \includegraphics[scale=0.72]{til.eps} \\
\includegraphics[scale=0.72]{mlton.eps} & 
\end{tabular}
\caption{Fraction of lines at least as wide as a given limit}
\label{fig:width}
\end{figure}

Figure~\ref{fig:width} shows the results of our analysis. In the SML/NJ project,
we notice an ``elbow'' in the plot around 80 columns. This inflection represents
an effort to limit the maximum width of lines. The tail of the inverse cumulative
histogram represents the failure to stay within the intended limit. We notice
also such as ``elbow'' in both the Til and the MLton projects, which we hypothesize
to mean that each also conforms generally to an 80-column limit. 

FoxNet a more interesting profile in its inverse cumulative histogram. We see
two discontinuities: one at 80 columns and another at 88 columns, another popular width limit.
However, since there are so few lines in the FoxNet project, the data produced are
not as smooth as in other projects. We must therefore be careful not to attribute
artificial discontinuities caused by data discreteness to actual features. More
investigation on this required.

Finally, the MLKit project is the most interesting in that we see no inflection in the
inverse histogram at all. This implies that the implementers of MLKit made little
effort to limit line width at all!

Clearly, not all experts conform to the 80-column rule.
In addition, even those we appear to have an 80-column rule ignore it approximately 1\% of the time.
Nevertheless, experts in most projects do seem to be trying to conform to some limit around 80 columns.
We therefore recommend to beginners also to try to stay within a column limit.
\subsection{Verbosity in \texttt{if} Expressions}
A convention cited in the Cornell style guide is that we should use \texttt{if}
expressions wisely. There are two specific uses of \texttt{if} that we
chose as violations.
\begin{enumerate}
\item Either one or both of the branches are boolean literals. We can rewrite the if expression with judicious use of \texttt{not}, \texttt{orelse}, and \texttt{andalso}
\item The test consists of \texttt{not} applied to another literal. We should just swap the two branches of the conditional.
\end{enumerate}
Rewriting these cases should help to improve readability and reduce verbosity in the code.

Table~\ref{table:style} shows the frequency of \texttt{if} violations in the five projects we selected.
There do not seem to be many violations -- on average approximately one every couple of thousand lines.
On inspection, the violations tend to fall into two groups: most are
missed opportunities to rewrite the \texttt{if} statement in potentially simpler way;
a few are made up of complicated \texttt{if} expressions where rewriting may not improve readability at all.
The first example below shows a \texttt{if} expression that could be rewritten as \texttt{e = t orelse f r}.
In the second case, the use of \texttt{orelse} does not significantly improve the readability or conciseness
of the expression. The problem is that the \texttt{if} expression has the same value as the test expression,
but if it evaluates to \texttt{false}, we also wish to evaluate some code for side-effects.

\begin{figure}[h!]
\begin{enumerate}
\item \texttt{if e = t then true else f r}
\item \texttt{if ar1 = ar2 then true else (}$\langle$\emph{code with side-effects}$\rangle$\texttt{; false)}
\end{enumerate}
\caption{Two \texttt{if} expressions from expert code where the \texttt{then} branch is the value \texttt{true}. In example 1, we could
rewrite in a more concise way by using \texttt{orelse}. In example 2, the use of \texttt{orelse} does not help much.}
\label{figure:ifthentrue}
\end{figure}

The expert code contains many uses of \texttt{orelse} and \texttt{andalso}, so we are forced to
conclude that the coders either missed an opportunity to use those syntactic constructs or chose
not to do so. But when we see in SML/NJ, MLKit, and Til an expression of the form:
\[\texttt{if }\langle expression\rangle\texttt{ then true else false}\]
we cannot help but wonder if it not just the former scenario. Despite the existence of
our selected \texttt{if} violations in expert code, we
recommend that beginners avoid verbosity in their \texttt{if} expressions by writing them
in more concise forms.

\begin{table}[h!]
\centering
\begin{tabular}{|l||c|c||>{\centering\arraybackslash\hspace{0pt}}p{1in}|c|}\hline
Project & \texttt{if} violations & Lines per instance & Missed fold opportunities & Lines per instance \\ \hline\hline
SML/NJ & 25 & 3244.1 & 75 & 1081.4 \\
MLKit & 70 & 4373.6 & 624 & 490.63 \\
FoxNet & 0 & -- & 9 & 911.00 \\
Til & 23 & 4774.5 & 181 & 140.43 \\
MLton & 30 & 1940.9 & 22 & 2646.6 \\ \hline
\end{tabular}
\caption{Selected usage style violations (see text)}
\label{table:style}
\end{table}

\subsection{Fold}\label{subsec:fold}
The Cornell and CMU style guides both suggest making heaving use of library functions.
The author has noted in discussions with the course staff for a programming languages course
that library functions for lists were frequent missed opportunities. We
chose to look at the \texttt{List} structure from the basis. Since many of the functions
from this structure, for example, \texttt{map}, \texttt{exists}, \texttt{all}, \texttt{filter},
and \texttt{rev}, can be written in terms of a fold over a list, we decided to look
for missed opportunities to apply a fold. \cite{Jeu13}
states that using higher order recursion schemes like a fold can allow a program
to benefit from shortcut fusion, and they
describe an algorithm for turning
explicitly recursive functions into folds. We chose not to tackle the more challenging
problem of finding all fold opportunities or transforming them,
but instead chose to look for a specific subset where folds might be applied.

Like \cite{Jeu13}, we are looking for cases of explicit recursion. Specifically, we try find
find instances of the following form:
\begin{center}
\texttt{fun f }\(e_{1}\ldots e_{i-1}\)\texttt{ (x::xs) }\(e_{i+1}\ldots e_{n}\) \(= e\)

where $e$ has the subexpression \texttt{f }\(v_{1}\ldots v_{i-1}\)\texttt{ xs }\(v_{i+1}\ldots v_{n}\)
\end{center}
That is, we're explicitly calling a recursive function on the tail of a input list.
The only other restriction is that the only list literal we allow in the same position
a declared pattern is the empty list literal (\texttt{[]}). This prevents idiosyncratic
behavior from being introduced into an otherwise sound definition by structural induction.
We won't concern ourselves with the order of the pattern match because either the patterns
are mutually exclusive (e.g., \texttt{[]} or a cons cell) or there is a redundant match
and the compiler will throw an error.

We see from Table~\ref{table:style} that missed opportunities for fold occur more often
than opportunities to rewrite \texttt{if} expressions. Perhaps this is
because \texttt{if} statements are relatively easy to reason about, but
rewriting a function as a fold over a list can be much more challenging.
And when a fold is not obvious, the programmer is surely tempted to just
get the job done with a recursive function definition.

On inspection, a good number of these fold opportunities cannot actually
be written as fold because on encountering the null list, an error or
some other imperative code is executed.

As we will see in Section~\ref{subsec:list}, folds occur quite often in
expert code, more often than the missed opportunities we were able to find.
That there are still a good number of unutilized fold opportunities
suggests that they were overlooked by their authors,
were left in place for a specific stylistic choice, or
written in explicit recursive form for reasons of efficiency.

Reviewers suggested that it is not always a good idea to transform
explicit recursion into a fold, particularly if the accumulator becomes
too complex. However, they suggested that map and filter which are
special cases of fold are always preferable to explicit recursion.

We propose to identify opportunities for map and filter
in a similar fashion to how we identified opportunities for fold.
Here, in the recursive call, we look for the appearance of the cons operator (\texttt{::})
immediately before the function name. In addition, we desire for the
null list to appear as the right-hand side of one of the clauses.
As before, we may not find all cases because of incorrect infix parsing.

\begin{table}[h!]
\centering
\begin{tabular}{|l||c|c|}\hline
Project & map/filter violations & Lines per instance \\ \hline\hline
SML/NJ & 2 & 40551.0 \\
MLKit & 76 & 4028.34 \\
FoxNet & 2 & 4099.50 \\
Til & 4 & 27453.2 \\
MLton & 1 & 58226.0 \\ \hline
\end{tabular}
\caption{Missed opportunities to use map or filter}
\label{table:mapfilter}
\end{table}

Table~\ref{table:mapfilter} shows the results of the analysis.
Very few opportunities for using either map or filter
exist. Indeed, the experts are very keen on using these fold variants
instead explicit of explicit recursion.

Our advice to beginners to is apply fold where it is reasonably
simple to do so, but always be on the lookout to use map or filter.
\subsection{Final thoughts on style}\label{subsec:stylethoughts}
We conclude this section with a few thoughts on style.
The takeaway for beginners from this section should not be
that because we see style violations in expert code,
style is not important. On the contrary, we believe that
these errors reflect the challenges that even experts
struggle with in the code, both in identifying places
whether style violations occur and finding consensus on
what good style is. Our suggestion to beginners is to
remember style's purpose of making code easier to read
and maintain, and then being as consistent as possible
in applying the style of his or her choice.
\section{SYNTACTIC FORMS}\label{sec:syntax}
\subsection{Imperative Features}\label{subsec:imper}
In Standard ML, there are two main imperative constructs in terms of expressions:
(1)~the \texttt{while} expression and (2)~a list of expression separated by semicolons (similar to the \texttt{begin...end} syntax of Pascal) \cite{Ull98}.
The former has the form \texttt{while <boolean-expression> do <expression>} and always returns a value of unit type.
The return value indicates that the \texttt{while} expression's purpose is to produce the side-effects associated
with the expression in its body. The sequence of expressions evaluates only to the last in that sequence, indicating
in a similar fashion, that all expressions in the list except for the last are evaluated for side-effects.

In addition, one could use the \texttt{val} declaration to perform imperative actions.
Generally, the results are assigned to unit or the wild card pattern:
\begin{center}
\texttt{val () = ...}\\
\texttt{val \_ = ...}
\end{center}
Of course, one of the paradigms in writing functional programs is to avoid side-effects.
Such a principled approach allows us to reason better about correctness and
removes the constraints imposed by having a fixed order of execution \cite{Hug90}. Therefore,
it behooves us to examine whether such imperative features are necessary or avoidable.
\subsubsection{Expressions with \texttt{while}}
\begin{table}[h!]
\centering
\begin{tabular}{|l||c|c|}
\hline
Project & Instances of \texttt{while} & Lines per instance \\ \hline\hline
SML/NJ & 0 & --\\
MLKit & 0 & --\\
FoxNet & 0 & -- \\
Til & 2 & 54906.5 \\
MLton & 0 & --\\ \hline
\end{tabular}
\caption{Occurrences of \texttt{while} expressions}
\label{table:while}
\end{table}
As we can see from Table~\ref{table:while}, all projects except for Til
contain no \texttt{while} loops at all.

In the two instances found in the Til project, the body of the \texttt{while}
loop includes modifying a mutable reference, which justifies the use of
an imperative feature. Getting rid of the while loops requires a design of how the code works so
that we no longer need to keep track of mutable state and thus, is not very straightforward.

Overall, we can observe that there are very few usages of \texttt{while} expressions
in expert projects.
From the analysis, expert ML programmers seemed quite principled in avoiding imperative
elements of ML. This is perhaps not surprising since they could have chosen
to implement in any language but selected a functional language in the end.
We therefore recommend that beginners avoid using \texttt{while} loops completely.
\subsubsection{Expression Lists}
\begin{table}[h!]
\centering
\begin{tabular}{|l||c|c|}
\hline
Project & Instances of expression lists & Lines per instance \\ \hline\hline
SML/NJ & 1970 & 41.17 \\
MLKit & 3869 & 79.13 \\
FoxNet & 68 & 120.6 \\
Til & 1928 & 59.96 \\
MLton & 641 & 90.84 \\ \hline
\end{tabular}
\caption{Occurrences of expressions lists}
\label{table:explist}
\end{table}
Table~\ref{table:explist} shows how often expression lists are used in our
examples of expert ML code. We find that the number of uses is significantly
greater than that of the \texttt{while} expression. The greatest number
of occurrences, normalized by the number of lines, appears to be in the SML/NJ
compiler where we find an expression list on average once every with every
40 or so lines of code.

The expression lists are likely used for the side-effects of the non-terminal
expression in the list. Sadly, the expressions are quite complex and difficult
to aggregate into meaningful groupings programmatically. Nevertheless,
looking at the examples of expression lists in the expert code, we find that
the expression lists frequently occur in the following contexts:
\begin{itemize}
\item[$\bullet$] pretty printing
\item[$\bullet$] outputting error messages
\item[$\bullet$] other types of I/O
\item[$\bullet$] assignment of reference variables (with \texttt{:=})
\item[$\bullet$] arrays
\item[$\bullet$] with the keyword \texttt{use} or the compilation manager to bring in other modules
\end{itemize}

Haskell has the IO monad to allow users to perform I/O in a purely functional way \cite{Jon93}.
Standard ML does not, so users are left to perform their
I/O unsafely in imperative fashion.

Arrays and references are two examples that \cite{Ull98} cites as way to ``violate the functional,
side-effect-free style,'' but also noting that ``there are situations where programs cannot
be made adequately efficient unless we are allowed to change some value bindings.''
\subsubsection{Unit \texttt{val} and wildcard \texttt{val}}
\begin{table}[h!]
\centering
\begin{tabular}{|l||c|c|}
\hline
Project & Instances of imperative \texttt{val} & Lines per instance \\ \hline\hline
SML/NJ & 296 & 273.99 \\
MLKit & 532 & 575.48 \\
FoxNet & 70 & 117.13 \\
Til & 427 & 257.17 \\
MLton & 731 & 79.653 \\ \hline
\end{tabular}
\caption{Occurrences of imperative \texttt{val} bindings}
\label{table:val}
\end{table}

Table~\ref{table:val} shows that imperative \texttt{val} bindings
occur with frequency comparable to that of expression lists.
Indeed, most of these occurrences are also for the purpose of I/O.
Therefore, we conclude that experts use imperative \texttt{val} bindings
to achieve execute one-time imperative code.
\subsubsection{Final thoughts on imperative features}
Imperative features are clearly unavoidable as evidenced in expert ML code. The experts do
limit their uses of expression lists to a few general cases, mostly dominated by I/O.
We conclude that the ML experts believe in the ``functional, side-effect-free'' style
and conform to it as much as possible. Beginners in ML are encouraged for follow their example.
\subsection{List Library Functions}\label{subsec:list}
We wanted to see if there was any discernable pattern in how expert programmers
used library functions, but not all projects require the same library.
It is difficult to imagine writing a substantial amount of code in ML without
relying heavily on lists, so we decided to just examine how expert programmers relied upon the functions
in the \texttt{List} structure from the standard basis.

\begin{table}[h!]
\centering
\begin{tabular}{|p{1in}||>{\centering\arraybackslash}p{0.62in}|>{\centering\arraybackslash\hspace{0pt}}p{0.9in}|>{\centering\arraybackslash\hspace{0pt}}p{0.6in}|>{\centering\arraybackslash\hspace{0pt}}p{0.5in}|>{\centering\arraybackslash\hspace{0pt}}p{0.6in}|}
\hline
Function & SML/NJ & MLKit & FoxNet & Til & MLton \\ \hline\hline
\texttt{::} & 882 & 6350 & 97 & 801 & 372 \\
\texttt{@} & \textbf{205} & \textbf{602} & \textbf{18} & \textbf{248} & \textbf{69} \\
\texttt{all} & 13 & 76 & 0 & 53 & 35 \\
\texttt{app} & \textbf{331} & \textbf{728} & 3 & \textbf{340} & \textbf{45} \\
\texttt{drop} & 0 & 28 & 0 & 7 & 2 \\
\texttt{exists} & 24 &  96 & 0 & 25 & 28 \\
\texttt{filter} & 17 & 138 & 3 & 4 & 8 \\
\texttt{find} & 48 & 130 & 0 & 48 & 13 \\
folds\hspace{2in}(\texttt{foldl},\texttt{foldr}) & \textbf{237} (89,\textbf{148}) & \textbf{920} (522,398) & \textbf{15}\hspace{1in}(7,8) & \textbf{171} (59,112) & \textbf{57} (30,27) \\
\texttt{getItem} & 0 & 20 & 0 & 0 & 2 \\
\texttt{hd} & 42 & 44 & 0 & 133 & 16 \\
\texttt{last} & 8 & 78 & 0 & 2 & 22 \\
\texttt{length} & \textbf{163} & 578 & 3 & 119 & \textbf{87} \\
\texttt{map} & \textbf{701} & \textbf{1724} & \textbf{60} & \textbf{1029} & \textbf{108} \\
\texttt{mapPartial} & 7 & 26 & 0 & 0 & 3 \\
\texttt{nth} & 5 & 4 & 0 & 6 & 8 \\
\texttt{null} & 8 & 50 & 0 & 1  & 20 \\
\texttt{partition} & 14  & 14 & 0 & 3 & 2 \\
\texttt{rev} & 137 & 472 & \textbf{13} & \textbf{178} & \textbf{120} \\
\texttt{revAppend} & 1 & 0 & 0 & 0 & 1 \\
\texttt{tabulate} & 1 & 18 & 0 & 0  & 13 \\
\texttt{take} & 6 & 10 & 0 & 10 & 1 \\
\texttt{tl} & 54 & 114 & 0 & \textbf{171} & 26 \\\hline\hline
Total & 2904 & 12220 & 212 & 3349 & 1058 \\
Lines per call & 27.928 & 25.054 & 38.674 & 32.790 & 55.034 \\\hline
\end{tabular}
\caption{Calls to functions from the \texttt{List} structure (\texttt{concat} omitted). Bolded entries each account for more than 5\% of the total number of occurrences in a project.}
\label{table:list}
\end{table}

Before we discuss the analysis, we should highlight some salient points.
Since the easiest way to create lists of arbitrary size is to the use the cons operator (\texttt{::}),
this operator naturally occurs the most, accounting for, in some cases, half
the occurrences of function calls. But we don't gain much insight from looking at this operator
because it is so common and critical to working with lists. It should also be noted
that we are excluding any occurrences of the cons operator used in pattern matching.
Additionally, we had trouble distinguishing uses of \texttt{concat} from the \texttt{List} structure
as opposed to the \texttt{String} structure. As a result, the occurrences of \texttt{concat} have been omitted.
Our results are tabulated in Table~\ref{table:list}.

It is amazing that list functions are used with almost the same frequency (normalized
by the number of lines) in every expert project. The normalized frequency
basically stay constant, relative to each other, even when we exclude the occurrences of the cons operator.

The values in bold indicate that a function accounts for more than 5\% of all function calls to the \texttt{List} structure.
We see that folds (both left and right), list concatenation (\texttt{@}), and \texttt{map} break the threshold in all 5 projects. Between them,
\texttt{map} is more common, occurring almost as frequency as the cons operator in some cases. This
suggests possibly that expert coders in ML are transforming data in bulk within a list container.

Other functions of note are \texttt{rev} and \texttt{app}, both of which each account for more than 5\% of
occurrences in four of five projects. The \texttt{app} is particularly interesting because it can be
considered an imperative version of \texttt{map}, which we have observed is called quite often. We hypothesize
that not only do programmer transform data in bulk as a list with \texttt{map}, the execute side-effects
in bulk in a list with \texttt{app}. The reversal function (\texttt{rev}) allows one to transform a list
to a dual form that may be more useful for a given application.

The pair of fold functions, taken together, break the threshold on all five projects.
This is perhaps not surprising since a fold is an excellent way to aggregate data in a list
into another value, possibly even another list. Indeed, many of the functions on lists
including \texttt{map} and \texttt{app} could be rewritten using a fold.

Finally, we point out that the Til project makes heavy use of the head (\texttt{hd}) and tail (\texttt{tl})
functions which respectively return the first element and all but the first element in a list.
That no other project makes heavy use of these two functions is interesting because from
the author's experience, extracting the head and tail of a list is a frequent necessity.
It is likely that the other four projects make heavy use of pattern matching on a cons cell (e.g., \texttt{x ::\ xs})
instead of explicitly call either \texttt{hd} or \texttt{tl}.

To conclude this section, we recommend to beginners to make heavy use of library functions
for lists. The experts particularly seem to like the list mapping and list contentation operators.
And despite our criticism of missed fold opportunities, folds are also quite common in code.
Based on a preference by coders on four of five projects, we recommend using pattern
matching to extract the head and tail of a list instead of explicit function calls.
\subsection{Program Hierarchy and Layout}\label{subsec:struct}
\subsubsection{Modularity and Abstraction}\label{subsubsec:modularity}
Program modularity eases the burden of maintenance and refactoring.
Abstraction provides a uniform interface, hides distracting implementation details,
and conceals sensitive information. With ML's module system, we can achieve
abtraction and modularity by using structures, signatures, and functors.

\begin{table}[h!]
\centering
\begin{tabular}{|l||>{\centering\arraybackslash}p{1.5in}|c||>{\centering\arraybackslash}p{1.5in}|c|}
\hline
Project & Files with module declarations & Fraction & Files with only module declarations & Fraction \\ \hline\hline
SML/NJ & 297 & 0.99 & 297 & 0.99 \\
MLKit & 1308 & 0.99 & 1270 & 0.96 \\
FoxNet & 30 & 0.42 & 29 & 0.40 \\
Til & 445 & 0.97 & 443 & 0.96 \\
MLton & 397 & 0.94 & 386 & 0.91 \\ \hline
\end{tabular}
\caption{Proportion of files that, at top level, contain module declarations or only contain module declarations.}
\label{table:module}
\end{table}

Table~\ref{table:module} shows what fraction of files define a structure, signature, or functor at top level
and what fraction of files define only a structure, signature, or functor at top level.
We find that in larger projects, almost every single file contains a functor, signature, or structure
definitions at top level. Nearly as many have at least one such declaration in a file.
The smaller FoxNet project stands out as having a smaller fraction in both cases.

We believe that these data reflect that as projects get large, expert ML coders
make very heavy use of the ML module system to help manage the complexity.
Due to the heavy reliance on the ML module system,
we suggest that beginners make heavy use of the ML module system
to impart modularity and some level of abstraction to their work, particularly
when the projects they are working on grow in size.
\subsubsection{Helper Functions in \texttt{let} Expressions and \texttt{local} Declarations}\label{subsubsec:let}
Helper functions allow programmers to refer to reoccurring code by name.
They can also help make the code more readable by giving complicated
lambda expressions a helpful name. We might consider looking at
how frequently our expert examples employ helper functions. Sadly,
we cannot infer from the name of a function or its body whether or not
a function is a helper function. But if a function appears inside
the list of declarations in a \texttt{let} expression or a \texttt{local}
declaration, one might hypothesize that it is likely a helper function
of some sort, since it is limited in scope to the body of the \texttt{let}
or \texttt{local} construction.

\begin{table}[h!]
\centering
\begin{tabular}{|l||>{\centering\arraybackslash}p{1.2in}|>{\centering\arraybackslash}p{1.2in}|c||>{\centering\arraybackslash}p{1.2in}|c|}
\hline
Project & Function declarations in \texttt{let} & Function declarations in \texttt{local} & Sum & Total Function Declarations & Fraction \\ \hline\hline
SML/NJ & 2652 & 192 & 2844 & 4866 & 0.58 \\
MLKit & 6130 & 2682 & 8812 & 24841 & 0.35 \\
FoxNet & 103 & 19 & 122 & 530 & 0.23 \\
Til & 1856 & 500 & 2356 & 5627 & 0.42 \\
MLton & 1004 & 252 & 1256 & 3533 & 0.36 \\\hline
\end{tabular}
\caption{Functions declared inside of \texttt{let} expressions}
\label{table:funlet}
\end{table}

Table~\ref{table:funlet} shows how functions are declared in side of \texttt{let} or \texttt{local}
constructs, and what fraction that represents out of all function declarations in the
entire project.
In the larger four projects, at least one-third of all function declarations occur
inside of a local or a let binding. As argued above, these are likely to be helper functions.
In other words, the experts are quite fond of helper functions if one out of every three functions
are helper functions.
Beginners are encouraged for follow their example.
\section{CONCLUSIONS AND CONTRIBUTIONS}\label{sec:future}
The key contributions of this project are three-fold. First, we
identified some features of ML expertise. These features are
selected not based on what experts recommend but rather on how they
write their own code.

Second, we've distilled some useful tips for novices based
on our discoveries. We've grouped these items into three main categories:
(1)~typographic structure, that is, how the code physically looks,
(2)~syntactic structure, that is, what and when certain language constructs are used in any source file, and
(3)~global structure, that is, how code is grouped and organized.
\begin{enumerate}
\item Typographic Structure
\begin{enumerate}
\item No consensus on tabs vs.\ spaces. Pick one and be consistent.
\item Try to keep under 80 columns of text, but it's OK go over occasionally.
\end{enumerate}
\item Syntactic Structure
\begin{enumerate}
\item Rewrite if expressions if one of the branches is a boolean literal or if the test is \texttt{not} applied a boolean expression
\item Folds are nice if you appropriate, but experts almost always use \texttt{map} and \texttt{filter} when they can.
\item Avoid while loops.
\item Expression lists are OK for I/O.
\item Imperative \texttt{val} bindings are OK for I/O.
\item Use list library functions when possible, especially \texttt{map} and folds.
\item Pattern match on lists instead of using \texttt{hd} or \texttt{tl}
\end{enumerate}
\item Global Structure
\begin{enumerate}
\item Use the ML module system.
\item Use helper functions.
\end{enumerate}
\end{enumerate}

Finally, we've created a set of tools that programmer of all levels
can use to evaluate how well they are performing on the above
eight points. The primary audience is intended to be novices,
who can receive feedback on how well the certain features
of their code mirror those of experts.
\section*{SUPPORTING CODE}
Supporting source code is available on GitHub via \url{http://www.github.com/zhelu/smlproject}.
\begin{thebibliography}{99}
\bibitem[Ayerbe and Vazquez 1998]{Aye98} Ayerbe, A., Vazquez, I. (1998) Software Products Quality Improvement with a Programming Style Guide and a Measurement Process. In \emph{Proceedings 22nd Annual International Computer Software and Applications Conference (COMPSAC'98)}. 172--178.
\bibitem[Binkley 2007]{Bin07} Binkley, D., 2007. Source Code Analysis: A Road Map. In \emph{Workshop on the Future of Software Engineering (FOSE'07)}. Minneapolis, MN. 104--119. Washington, D.C.: IEEE Computer Society.
\bibitem[Bouwers et al. 2012]{Bou12} Bouwers, E., Visser, J., van Deursen, A. (2012) Getting what you measure. \emph{Commun. ACM}, \textbf{55}, 54--59.
\bibitem[CMU 2012]{Cmu12} CMU Computer Science Department. (2012) SML Style Guide. Accessed on 14 Nov 2013 from \url{http://www.cs.cmu.edu/~15150/resources/style.pdf}
\bibitem[Gladwell 2008]{Gla08} Gladwell, M. (2008) \emph{Outliers}. Little, Brown and Co., New York, NY.
\bibitem[Hughes 1990]{Hug90} Hughes, J. (1990) Why Functional Programming Matters. In D.\,A.\,Turner, ed, \emph{Research Topics in Functional Programming}. Boston, MA: Addison Wesley.
\bibitem[Jones 1994]{Jon94} Jones, C. (1994) Software metrics: good, bad and missing. \emph{Computers}, \textbf{27}, 98--100.
\bibitem[Ordonez and Hadda 2008]{Ord08} Ordonez, M.\,J. Hadda, H.\,M. (2008) The State of Metrics in Software Industry. In \emph{5th International Conference Information Technology: New Generations (ITNG'08)} Las Vegas, NV. 453--458. Washington, D.C.: IEEE Computer Society.
\bibitem[Peyton Jones, Wadler 1993]{Jon93} Peyton Jones, S.\,L., Wadler, P. (1993) Imperative Functional Programming. In \emph{ACM Symposium on Principles of Programming Languages (POPL'93)}. Charleston, SC. 71--84. New York, NY: ACM.
\bibitem[Takai et al. 2011]{Tak11} Takai, Y., Kobayashi, T., Agusa, K. (2011) Software Metrics Based on Coding Standards Violations. In \emph{2011 Joint Conference of the 21st Int'l Workshop on Software Measurement and 6th Int'l Conference on Software Process and Product Measurement (IWSM-MENSURA)} Nara, Japan. 273--278. Washington, D.C.: IEEE Computer Society.
\bibitem[Ullman 1998]{Ull98} Ullman, J.\,D. (1998) \emph{Elements of ML Programming} (ML97 Ed). Prentice Hall, Upper Saddle River, NJ.
\bibitem[Van der Jeugt et al. 2013]{Jeu13} Van der Jeugt, J., Keuchel, S., Schrijvers, T. (2013) Bringing Functions into the Fold. Unpublished manuscript.
\bibitem[Vogelsang et al. 2010]{Vog10} Vogelsang, S., Fehnker, A., Huuck, R., Reif, W. (2010) Software Metrics in Static Program Analysis. In \emph{Formal Methods and Software Engineering. Proceedings 12th International Conference on Formal Engineering Methods (ICFEM'10)}. Shanghai, China. 485--500. Heidelberg, Germany: Springer.
\end{thebibliography}
\end{document}
